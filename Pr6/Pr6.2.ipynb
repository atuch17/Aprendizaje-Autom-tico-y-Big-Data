{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA 6 - VÍCTOR CHOZA MERINO - ADRIÁN TURIEL CHARRO\n",
    "## Parte 2 - Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar aqui al final si eso\n",
    "import scipy.io as sio\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import nltk.stem.porter\n",
    "\n",
    "import codecs\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones del profe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo 1\n",
    "def getVocabDict(reverse=False):\n",
    "    \"\"\"\n",
    "    Function to read in the supplied vocab list text file into a dictionary.\n",
    "    Dictionary key is the stemmed word, value is the index in the text file\n",
    "    If \"reverse\", the keys and values are switched.\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    with open(\"p6/vocab.txt\") as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            if not reverse:\n",
    "                vocab_dict[key] = int(val)\n",
    "            else:\n",
    "                vocab_dict[int(val)] = key\n",
    "\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo 2\n",
    "def preProcess(email):\n",
    "    \n",
    "    hdrstart = email.find(\"\\n\\n\")\n",
    "    if hdrstart != -1:\n",
    "        email = email[hdrstart:]\n",
    "\n",
    "    email = email.lower()\n",
    "    # Strip html tags. replace with a space\n",
    "    email = re.sub('<[^<>]+>', ' ', email)\n",
    "    # Any numbers get replaced with the string 'number'\n",
    "    email = re.sub('[0-9]+', 'number', email)\n",
    "    # Anything starting with http or https:// replaced with 'httpaddr'\n",
    "    email = re.sub('(http|https)://[^\\s]*', 'httpaddr', email)\n",
    "    # Strings with \"@\" in the middle are considered emails --> 'emailaddr'\n",
    "    email = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email)\n",
    "    # The '$' sign gets replaced with 'dollar'\n",
    "    email = re.sub('[$]+', 'dollar', email)\n",
    "    return email\n",
    "\n",
    "\n",
    "def email2TokenList(raw_email):\n",
    "    \"\"\"\n",
    "    Function that takes in a raw email, preprocesses it, tokenizes it,\n",
    "    stems each word, and returns a list of tokens in the e-mail\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    email = preProcess(raw_email)\n",
    "\n",
    "    # Split the e-mail into individual words (tokens) \n",
    "    tokens = re.split('[ \\@\\$\\/\\#\\.\\-\\:\\&\\*\\+\\=\\[\\]\\?\\!\\(\\)\\{\\}\\,\\'\\\"\\>\\_\\<\\;\\%]',\n",
    "                      email)\n",
    "\n",
    "    # Loop over each token and use a stemmer to shorten it\n",
    "    tokenlist = []\n",
    "    for token in tokens:\n",
    "\n",
    "        token = re.sub('[^a-zA-Z0-9]', '', token)\n",
    "        stemmed = stemmer.stem(token)\n",
    "        #Throw out empty tokens\n",
    "        if not len(token):\n",
    "            continue\n",
    "        # Store a list of all unique stemmed words\n",
    "        tokenlist.append(stemmed)\n",
    "\n",
    "    return tokenlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = getVocabDict()\n",
    "#vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marcandoPalabras(filename, valor):\n",
    "    \n",
    "    X = np.zeros([len(filename),len(vocabulario)])\n",
    "    y = np.full(len(filename), valor)\n",
    "    \n",
    "    # Creamos una matriz donde cada fila equivale a un correo\n",
    "    for j,file in enumerate(filename):\n",
    "        # Leemos el correo\n",
    "        email_contents = codecs.open(file,'r', encoding='utf8', errors='ignore').read()\n",
    "        # Creamos una lista de palabras\n",
    "        email_contents = email2TokenList(email_contents)\n",
    "                \n",
    "        # Marco con 1 si la palabra aparece en el correo\n",
    "        for i,voc in enumerate(vocabulario.keys()):\n",
    "            if voc in email_contents:\n",
    "                X[j,i]=1       \n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1899)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crea una lista con todos los nombres de los ficheros dentro de una ruta\n",
    "filesList = np.array(glob.glob('p6/spam/*.txt'))\n",
    "X1, y1 = marcandoPalabras(filesList, 1) # SPAM\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 1899)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesList = np.array(glob.glob('p6/hard_ham/*.txt'))\n",
    "X2, y2 = marcandoPalabras(filesList, 0) # NO SPAM\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2551, 1899)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesList = np.array(glob.glob('p6/easy_ham/*.txt'))\n",
    "X3, y3 = marcandoPalabras(filesList, 0) # NO SPAM\n",
    "X3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3301, 1899)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SuperX = np.concatenate([X1,X2,X3])\n",
    "SuperX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3301,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SuperY = np.concatenate([y1,y2,y3])\n",
    "SuperY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partimos el conjunto de datos en entrenamiento (70%) y test (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    SuperX, SuperY, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear SVM (Spam Classification)...\n",
      "C= 0.01\n",
      "Precisión de entrenamiento: 0.9798183652875883\n",
      "C= 0.03\n",
      "Precisión de entrenamiento: 0.9798183652875883\n",
      "C= 0.1\n",
      "Precisión de entrenamiento: 0.9798183652875883\n",
      "C= 0.3\n",
      "Precisión de entrenamiento: 0.9757820383451059\n",
      "C= 1\n",
      "Precisión de entrenamiento: 0.9788092835519677\n",
      "C= 3\n",
      "Precisión de entrenamiento: 0.9717457114026236\n",
      "C= 10\n",
      "Precisión de entrenamiento: 0.9677093844601413\n",
      "C= 30\n",
      "Precisión de entrenamiento: 0.9677093844601413\n"
     ]
    }
   ],
   "source": [
    "print ('Training Linear SVM (Spam Classification)...')\n",
    "C_vec = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n",
    "\n",
    "for i, C_i in enumerate(C_vec):\n",
    "    svm1 = SVC(kernel='linear', C=C_i)\n",
    "    svm1.fit(X_train, y_train.ravel())\n",
    "    p = accuracy_score(y_test, svm1.predict(X_test))\n",
    "    print('C=',C_i)\n",
    "    print ('Precisión de entrenamiento:', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C= 0.3  sigma= 3\n",
      "0.8506559031281534\n",
      "C= 0.3  sigma= 10\n",
      "0.9182643794147326\n",
      "C= 0.3  sigma= 30\n",
      "0.8244197780020182\n",
      "C= 1  sigma= 3\n",
      "0.9112008072653885\n",
      "C= 1  sigma= 10\n",
      "0.9757820383451059\n",
      "C= 1  sigma= 30\n",
      "0.8859737638748738\n",
      "C= 3  sigma= 3\n",
      "0.9212916246215943\n",
      "C= 3  sigma= 10\n",
      "0.9798183652875883\n",
      "C= 3  sigma= 30\n",
      "0.9475277497477296\n",
      "C= 10  sigma= 3\n",
      "0.9202825428859738\n",
      "C= 10  sigma= 10\n",
      "0.9798183652875883\n",
      "C= 10  sigma= 30\n",
      "0.9808274470232089\n",
      "C= 30  sigma= 3\n",
      "0.9192734611503531\n",
      "C= 30  sigma= 10\n",
      "0.9778002018163471\n",
      "C= 30  sigma= 30\n",
      "0.9798183652875883\n"
     ]
    }
   ],
   "source": [
    "C_vec = [0.3, 1, 3, 10, 30]\n",
    "sigma_vec = [3, 10, 30]\n",
    "\n",
    "for i, C_i in enumerate(C_vec):\n",
    "    for j, sigma_j in enumerate (sigma_vec):\n",
    "        svm3 =  SVC(kernel='rbf', C=C_i, gamma=1/(2*sigma_j**2))\n",
    "        svm3.fit(X_train, y_train.ravel())\n",
    "        print('C=',C_i,' sigma=',sigma_j)\n",
    "        print(accuracy_score(y_test, svm3.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
